{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Cloud ML Engineを利用して機械学習をスケールアップする </h1>\n",
    "\n",
    "このnotebookでは、これまで作成してきたタクシーの乗車料金を予測するTensorFlowのモデルをCloud ML Engineで利用できるようにパッケージ化する方法を学びます。<br>\n",
    "まずは、小さなデータセットから始めていきましょう。\n",
    "\n",
    "このコースの後半では、より効果的な機械学習モデルの作成方法について確認していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> プロジェクトとバケットを環境変数として入力する</h2>\n",
    "\n",
    "注意事項:\n",
    "<ol>\n",
    "<li> Project IDは使用するプロジェクトを表す*ユニーク*な文字列です（プロジェクト名とは異なります）。GCPコンソールのホーム画面から確認することができます。 </li>\n",
    "<li> クラウド上での機械学習のトレーニングでは、モデルファイルを保存したり復元したりというプロセスを含んでいます。まだバケットを持っていない場合、GCPコンソールからバケットを一つ作成してください。Project IDを先頭につけることでユニークなバケット名をつけることが一般的です。またコスト上の理由から、単一のRegionでバケットを作成するのがよいでしょう。</li>\n",
    "</ol>\n",
    "自分のProject IDとバケット名に<b>以下のセルの変数を変更してください。</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos' # ご自分のProject IDに変更してください。\n",
    "REGION = 'us-central1' # AI Platformを利用できるリージョンを選択してください https://cloud.google.com/ml-engine/docs/regions\n",
    "BUCKET = 'cloud-training-demos-ml' # ご自分のバケット名に変更してください。選択したリージョンのリージョナルなバケットを使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.4'  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングデータを含むバケットの読み書きができるように、Cloud ML Engineサービスアカウントを認証します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print(response['serviceAccount'])\")\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> コードをパッケージ化する</h2>\n",
    "\n",
    "コードをPythonの標準的なパッケージ構造にしましょう。<a href=\"taxifare/trainer/model.py\">model.py</a>と<a href=\"taxifare/trainer/task.py\">task.py</a>にはこれまでに扱ったTensorFlowコードが格納されています。taxifare/trainerディレクトリの中の構造を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find taxifare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> データへの絶対パスを確認する</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の絶対パスに注目してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo $PWD\n",
    "rm -rf $PWD/taxi_trained\n",
    "cp $PWD/../tensorflow/taxi-train.csv .\n",
    "cp $PWD/../tensorflow/taxi-valid.csv .\n",
    "head -1 $PWD/taxi-train.csv\n",
    "head -1 $PWD/taxi-valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pythonモジュールをコマンドラインから実行する</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "python -m trainer.task \\\n",
    "   --train_data_paths=\"${PWD}/taxi-train*\" \\\n",
    "   --eval_data_paths=${PWD}/taxi-valid.csv  \\\n",
    "   --output_dir=${PWD}/taxi_trained \\\n",
    "   --train_steps=1000 --job-dir=./tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls $PWD/taxi_trained/export/exporter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./test.json\n",
    "{\"pickuplon\": -73.885262,\"pickuplat\": 40.773008,\"dropofflon\": -73.987232,\"dropofflat\": 40.732403,\"passengers\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## local predict doesn't work with Python 3 yet\n",
    "#%bash\n",
    "#model_dir=$(ls ${PWD}/taxi_trained/export/exporter)\n",
    "#gcloud ai-platform local predict \\\n",
    "#    --model-dir=${PWD}/taxi_trained/export/exporter/${model_dir} \\\n",
    "#    --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> gcloudコマンドを利用してローカルで実行する</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/taxifare/trainer \\\n",
    "   -- \\\n",
    "   --train_data_paths=${PWD}/taxi-train.csv \\\n",
    "   --eval_data_paths=${PWD}/taxi-valid.csv  \\\n",
    "   --train_steps=1000 \\\n",
    "   --output_dir=${PWD}/taxi_trained "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムシードの影響で結果は異なりますが、私が実行した際には```average_loss```は187程度になりました。つまり、RMSEは13前後となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のステップでTensorboardで学習の過程を確認してみましょう。\n",
    "<ol>\n",
    "<li>左上のファイルアイコン -> \"+\" をクリックする\n",
    "<li>Tensorboardをクリックする\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $PWD/taxi_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> gcloudコマンドで学習ジョブを送信する</h2>\n",
    "\n",
    "はじめに、学習データをクラウドにコピーしましょう。コピーができたら学習ジョブを実行します。\n",
    "\n",
    "ジョブを送信できたら、コンソールから <b>AI Platform -> Jobs</b>に移動し、ジョブの進捗をモニタリングしましょう\n",
    "\n",
    "<b>注意：</b>notebookのセルが実行中（青のプログレスバーが表示される）で止まったり、認証トークンのリフレッシュができないなどのエラーが返ってきても気にしないでください。<br>\n",
    "ジョブ自体はクラウド上で実行されているので、コンソールからジョブのモニタリングを行ってください\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo $BUCKET\n",
    "gsutil -m rm -rf gs://${BUCKET}/taxifare/smallinput/\n",
    "gsutil -m cp ${PWD}/*.csv gs://${BUCKET}/taxifare/smallinput/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/taxifare/smallinput/taxi_trained\n",
    "JOBNAME=lab3a_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/taxifare/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/taxifare/smallinput/taxi-train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/taxifare/smallinput/taxi-valid*\"  \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> モデルをデプロイする </h2>\n",
    "\n",
    "モデルが保存されいるディレクトリを指定して、モデルのデプロイに使用しましょう。モデルのデプロイには<b>5分</b>ほどかかる場合があります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/taxifare/smallinput/taxi_trained/export/exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"taxifare\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/taxifare/smallinput/taxi_trained/export/exporter | tail -1)\n",
    "echo \"Run these commands one-by-one (the very first time, you'll create a model and then create a version)\"\n",
    "#gcloud ai-platform versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ai-platform models delete ${MODEL_NAME}\n",
    "gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboardを開く\n",
    "\n",
    "<ol>\n",
    "<li>以下を実行し、出力をCloud Shellで実行する\n",
    "<li>Cloud Shell上のWeb Previewをクリックする\n",
    "<li>Preview on port 8080をクリックする\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR='gs://{0}/taxifare/smallinput/taxi_trained'.format(BUCKET)\n",
    "print('tensorboard --port=8080 --logdir={0}'.format(OUTDIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 予測を行う</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ai-platform predict --model=taxifare --version=v1 --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "\n",
    "request_data = {'instances':\n",
    "  [\n",
    "      {\n",
    "        'pickuplon': -73.885262,\n",
    "        'pickuplat': 40.773008,\n",
    "        'dropofflon': -73.987232,\n",
    "        'dropofflat': 40.732403,\n",
    "        'passengers': 2,\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'taxifare', 'v1')\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "print(\"response={0}\".format(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 大規模なデータセットで学習を行う</h2>\n",
    "\n",
    "以下のステップはすでに実行済みでデータセットのファイルを利用することができますの。そのため<b>以下のコメントに記載されているステップを実行する必要はありません。</b><br>\n",
    "次の章（特徴量エンジニアリング）では、同様の前処理をCloud Dataflowを用いて行います。\n",
    "\n",
    "http://bigquery.cloud.google.com/ に行って以下のクエリを入力してください\n",
    "\n",
    "<pre>\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "  'nokeyindata' AS key\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  AND ABS(HASH(pickup_datetime)) % 1000 == 1\n",
    "</pre>\n",
    "\n",
    "100万行のデータを取得することに注意してください。これを以下のステップでCSVファイルとしてエクスポートします。<br>\n",
    "（<b>このステップはすでに実行済みで、結果はGCSでパブリックにアクセスできるようになっています。</b>そのため実行する必要はありません）\n",
    "\n",
    "<ol>\n",
    "<li> \"Save As Table\"ボタンをクリックし、データセット名とテーブル名を指定します。\n",
    "<li> BigQueryコンソール上で、左側のメニューから新しく追加されたテーブルを見つけ、クリックします。\n",
    "<li> \"Export Table\"をクリックします\n",
    "<li> バケット名を指定し、train.csvとファイル名を指定します。 (例: gs://cloud-training-demos-ml/taxifare/ch3/train.csv). ジョブが完了するのを待ちます。（左側のメニューの\"Job History\"を見てみましょう。）\n",
    "<li> 上のクエリから、最後の\"== 1\"を\"== 2\"に変更し、Cloud Storageにvalid.csvというファイル名でエクスポートしましょう (e.g.  gs://cloud-training-demos-ml/taxifare/ch3/valid.csv)\n",
    "<li> 2つのファイルをダウンロードし、ヘッダー行を削除して再アップロードします。\n",
    "</ol>\n",
    "\n",
    "<p/>\n",
    "<p/>\n",
    "\n",
    "<h2> 100万行のデータセットを使用してクラウド上での学習を行う</h2>\n",
    "\n",
    "100万行のデータの学習には60分ほど時間がかかります。モデルは先程のものと全く同じものです。唯一の違いは入力（大規模なデータセットを利用）と、Cloud ML Engineのtier（BASICではなくSTANDARD_1を利用します。STANDARD_1はBASICのおよそ10倍の性能があります）です。<br>\n",
    "学習の最後のlossは32ですが、RMSE（検証データセットに対して算出）は変わらず9.03です。つまり、単にデータを足すだけではモデルの精度は上がらないようです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## this takes 60 minutes. \n",
    "\n",
    "OUTDIR=gs://${BUCKET}/taxifare/ch3/taxi_trained\n",
    "JOBNAME=lab3a_$(date -u +%y%m%d_%H%M%S)\n",
    "CRS_BUCKET=cloud-training-demos # use the already exported data\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/taxifare/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${CRS_BUCKET}/taxifare/ch3/train.csv\" \\\n",
    "   --eval_data_paths=\"gs://${CRS_BUCKET}/taxifare/ch3/valid.csv\"  \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
